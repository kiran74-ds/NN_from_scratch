{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Neural Networks that vaguely mimic the process of how the brain works with neurons that fire bits of information</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets take an example of classification where we have data with two class labels and data can be seperated linearly\n",
    "\n",
    "Linear Boundaries\n",
    "\n",
    "        Prediction :1 if Wx + b >= 0 \n",
    "                    0 if Wx + b < 0       \n",
    "             \n",
    "In 2-dimensional data our classification Boundary is Line<br>\n",
    "In 3-dimensional data our classification Boundary is Plane<br>\n",
    "In n-dimensional data our classification Boundary is n-1 dimensional hyperplane<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "its a building of block of neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.5"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Example of neuron code\n",
    "inputs = [1, 3, 5]\n",
    "weights = [0.4, 0.7, 0.6]\n",
    "bias = 2\n",
    "\n",
    "output = inputs[0]*weights[0] + inputs[1]*weights[1] + inputs[2]*weights[2] + bias\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perceptrons Algorithm\n",
    "\n",
    "+ Start with random weights and biases\n",
    "+ For every misclassification \n",
    "       if prediction is 0 \n",
    "           then we update weights\n",
    "            Change W = W + lr*(xi)\n",
    "            Change b = b + lr\n",
    "       if prediction is 1 \n",
    "           then we update weights\n",
    "            Change W = W - lr*(xi)\n",
    "            Change b = b - lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def prediction(X, W, b):\n",
    "    if np.matmul(X, W)+ b >= 0:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "def perceptron(X, y, W, b, lr):\n",
    "    for i in range(len(X)):\n",
    "        y_hat = prediction(X[i], W, b)\n",
    "        if y[i] - y_hat == 1:\n",
    "            W[0] += lr*X[i][0]\n",
    "            W[1] += lr*X[i][1]\n",
    "            b += lr\n",
    "        if y[i] - y_hat == -1:\n",
    "            W[0] -= lr*X[i][0]\n",
    "            W[1] -= lr*X[i][1]\n",
    "            b -= lr\n",
    "        \n",
    "    return W, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([(0, 0), (0, 1), (1, 0), (1, 1)])\n",
    "y = np.array([0, 0, 0, 1])\n",
    "W = np.array(np.random.rand(2,1))\n",
    "b = 0.1\n",
    "for i in range(100):\n",
    "    W, b = perceptron(X, y ,W, b, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction value for input [0 0] is 0\n",
      "prediction value for input [0 1] is 0\n",
      "prediction value for input [1 0] is 0\n",
      "prediction value for input [1 1] is 1\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(X)):\n",
    "    print(\"prediction value for input {} is {}\".format(X[i],prediction(X[i], W, b)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Error functions\n",
    "\n",
    "The error is what's telling how badly we are doing at that moment and how far we are from an ideal solution and if we constantly take steps to decrease the error then we will eventually solve our problem.\n",
    "\n",
    "In order to do gradient descent our error function can not be discrete , it should be <b>continous</b> and our error function needs to be <b>differentiable</b>\n",
    "\n",
    "In order to perform gradient descent our predictions should be also continous, the way we move to continous predictions from discrete predictions is to simply change the activation function from the step function to sigmoid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sigmoid function\n",
    "\n",
    "The sigmoid function is defined as sigmoid(x) = 1/(1+e-x).\n",
    "\n",
    "it returns the probabilty values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Softmax Function\n",
    "\n",
    "Lets say we have N classes and a linear model that gives us the following scores\n",
    "\n",
    "Linear Function :\n",
    "Scores :z1, z2, ...zn\n",
    "\n",
    "p(class i) = e^zi/(sum(e^z1+...e^zn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(scores):\n",
    "    probs = []\n",
    "    sum_exp = sum(np.exp(scores))\n",
    "    for i in range(len(scores)):\n",
    "        probs.append(np.exp(scores[i])/(sum_exp))\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.24472847105479764, 0.6652409557748219, 0.09003057317038046]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax([1, 2, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One Hot Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All our algorithms are numerical this means we need to input numbers but input data not always have numbers we use one hot encoding for categorical variables to represent in to numeric way"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Maximum Likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we do is we pick the model that gives the existing labels the highest probability thus, by maximizing the probability\n",
    "\n",
    "our prediction is y = g(Wx+b)<br>\n",
    "probabilty of four points in model 1 :<br>\n",
    "    p(r1)*p(r2)*p(b1)*p(b2) = 0.1*0.7*0.6*0.2 = 0.0084\n",
    "    \n",
    "probabilty of four points in model 2 :<br>\n",
    "    p(r1)*p(r2)*p(b1)*p(b2) = 0.8*0.6*0.7*0.9 = 0.3024\n",
    "    \n",
    "The model classifies most points correctly with p(all) indicating how accurate model is\n",
    "\n",
    "Instead of multiplying probabilities we will add them using ln(0.1*0.7*0.6*0.2) = ln(0.1)+ln(0.7)+ln(0.6)+ln(0.2)\n",
    "\n",
    "cross entropy of model1 = ln(0.1)+ln(0.7)+ln(0.6)+ln(0.2) = 4.8\n",
    "\n",
    "cross entropy of model2 = ln(0.8)+ln(0.6)+ln(0.7)+ln(0.9) = 1.2\n",
    "\n",
    "Good model is the one which has low cross entropy\n",
    "\n",
    "<b>Minimizing cross entropy</b>\n",
    "\n",
    "Cross entropy really says the following , if i have a bunch of events and a bunch of probabilities how likely is it that those events happen based on the probabilities?<br>\n",
    "If its very likely we have a small cross entropy<br>\n",
    "If its unlikely we have a large cross entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross entropy formula\n",
    "\n",
    "-yi*ln(pi)-(1-yi)*ln(1-pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(y, p):\n",
    "    y = np.array(y, dtype='float')\n",
    "    p = np.array(p, dtype='float')\n",
    "    assert len(p) == len(y)\n",
    "    \n",
    "    return -np.sum(y * np.log(p) + (1 - y) * np.log(1 - p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.115995809754082"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_entropy([0,0,1], [0.8, 0.7, 0.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6851790109107685"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_entropy([1,1,0], [0.8, 0.7, 0.1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multi-class Cross entropy formula\n",
    "\n",
    "CE = - sum(i)sum(j)yij*ln(pij)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Non Linear data \n",
    "\n",
    "We can combine linear models to create a non linear model\n",
    "we calculate probabilty for one of them, probability for the other then add them and then we apply sigmoid function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neural Network Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Network contains these layers and each layers contains n number of nodes\n",
    "+ Input \n",
    "+ Hidden Layer\n",
    "+ Output Layer\n",
    "\n",
    "\n",
    "when we more hidden layers then we called neural networks are called deep neural networks\n",
    "\n",
    "<img src='images/nn1.png' width=600>\n",
    "\n",
    "\n",
    "<img src='images/nn2.png' width=600>\n",
    "\n",
    "\n",
    "<img src='images/nn3.png' width=600>\n",
    "\n",
    "\n",
    "<b> Binary Classification </b>\n",
    "<img src='images/binary_classification.png' width=600>\n",
    "\n",
    "<b> Multi Class Classification </b>\n",
    "<img src='images/multiclass_classification.png' width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feed Forward\n",
    "\n",
    "+ Deep feedforward networks, also called feedforward neural networks, or multilayer perceptrons(MLPs)\n",
    "\n",
    "+ The goal of a feedforward network is to approximate some function f∗. \n",
    "    For example,for a classiﬁer,y=f(x) maps an input x to a category y. \n",
    "    \n",
    "+ A feedforward networkdeﬁnes a mappingy=f(x;θ) and learns the value of the parameters θ that result in the best function approximation.\n",
    "\n",
    "+ These models are called feedforward because information ﬂows through the function being evaluated from x, through the intermediate computations used to deﬁne f, and ﬁnally to the output y. There are nofeedbackconnections in which outputs of the model are fed back into itself. When feedforward neural networksare extended to include feedback connections, they are calledrecurrent neuralnetworks\n",
    "\n",
    "+ Feedforward neural networks are called networks because they are typically represented by composing together many different functions.For example, we might have three functions f(1),f(2), andf(3)connected in a chain, to form f(x) =f(3)(f(2)(f(1)(x))). These chain structures are the most commonly used structures of neural networks. In this case,f(1)is called the first layer of the network,f(2)is called the second layer, and so on.\n",
    "\n",
    "<img src='images/feedforward.png' width=800>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Back Propagation\n",
    "\n",
    "Now, we're ready to get our hands into training a neural network. For this, we'll use the method known as backpropagation. In a nutshell, backpropagation will consist of:\n",
    "\n",
    "+ Doing a feedforward operation.\n",
    "+ Comparing the output of the model with the desired output.\n",
    "+ Calculating the error.\n",
    "+ Running the feedforward operation backwards (backpropagation) to spread the error to each of the weights.\n",
    "+ Use this to update the weights, and get a better model.\n",
    "+ Continue this until we have a model that is good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overfitting and Underfitting\n",
    "\n",
    "<img src=\"images/model_errors.png\" width=800>\n",
    "\n",
    "\n",
    "Overfitting means model will generalize well \n",
    "\n",
    "Two solutions for the overfitting problem:\n",
    "  + Early Stopping\n",
    "  + Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Early Stopping\n",
    "\n",
    "We do gradient testing until the testing error stops decreasing and start to increase at that moment we stop, this algorithm is called early stopping\n",
    "\n",
    "Draw back is our training set will be low as we need to keep some validation data set\n",
    "\n",
    "<img src=\"images/early_stopping.png\" width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                        \"The Whole problem with the\n",
    "               Artificial Intelligence is that bad models \n",
    "               are so certain of themselves, and good models \n",
    "                           so full of doubts\"\n",
    "                                           BertrAIND Russell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regularization\n",
    "\n",
    "When we have large coefficients ,the model overfits<br>\n",
    "Imposing constraint such as penalizing large weights can reduce overfitting \n",
    "\n",
    "There are two types:<br>\n",
    "L1 - Lasso (add the absolute sum of the coefficients to the error along with the lambda)<br>\n",
    "       L1 is good for feature selection<br>\n",
    "       L1 produces values like (1,0)\n",
    "L2 - Ridge(add the square sum of the coefficients to the error along with the lambda)<br>\n",
    "       L2 is better for training models\n",
    "       L2 produces values like (0.5,0.5)\n",
    "       \n",
    "Absolute sum of (1,0) and (0.5, 0.5) are same but square sum is low for (0.5, 0.5)(0.25+0.25=0.5) than (1)(1+0) thus L2 prefers (0.5, 0.5) because it produces smaller sum of squares in turns smaller error "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dropout\n",
    "\n",
    "It is another way to overcome the overfitting.\n",
    "\n",
    "when we train neural networks sometimes one part of the network has very large weights and it endsup dominating all the training while the other part of the network doesnot get train.\n",
    "\n",
    "So as we go through the training in each epoch we randomly turn off some of the nodes in the network and will give opportunity for all the nodes to learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Local Minima\n",
    "<img src=\"images/local_minima.png\" width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Restart\n",
    "\n",
    "Sometimes our error can stuck at local minima to overcome this we can randomly initialize the weights mutiple times and see if we can get minimum error.\n",
    "This increases the probability that we will get to the global minima or at least pretty good local minima\n",
    "\n",
    "<img src=\"images/random_restart.png\" width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vanishing Gradient\n",
    "\n",
    "Gradients are calculated by continous mutiplication of derivates, the value of derivates of sigmoid activation function is small at the left and right which causes continous multiplication value to too tiny and gradient to practically vanish\n",
    "\n",
    "<img src=\"images/sigmoid.png\" width=600>\n",
    "\n",
    "\n",
    "<img src=\"images/vanishing_gradient.png\" width=600>\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"images/tanh.png\" width=600>\n",
    "<b>Tanh is better than sigmoid but both sigmoid and tanh activation functions can suffer from vanishing gradients because the derivates to the left and right is almost zero if there is no derivate then there will be no direction to move </b>\n",
    "\n",
    "<b>RELU activation function can comes to rescue here as the derivative of the relu is linear </b>\n",
    "<img src=\"images/relu.png\" width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploding Gradient\n",
    "\n",
    "We use gradient clipping for exploding gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batch and Stochastic Gradient Descent\n",
    "\n",
    "<b>Batch Gradient Descent</b>:\n",
    "In batchGD we update the weights in each step by running through entire data it takes huge computations as we need to run entire data on every step\n",
    "\n",
    "<b>SGD</b>:\n",
    "it takes the small subsets of the data , run them through the neural network , calculate the gradient of the error function based on those points and then move one step in that direction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learning Rate Decay\n",
    "<img src=\"images/learning_rate.png\" width=600>\n",
    "\n",
    "\n",
    "<img src=\"images/learning_rate_decay.png\" width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Momentum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/momentum.png\" width=600>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
